<!doctype html>
<html lang="en">
<head>
<title>Dendritic Trees</title>
<meta property="og:title" content=Dendritic Trees" />
<meta name="twitter:title" content="Dendritic Trees" />
<meta name="description" content="In this project, we investigate biologically inspired dendritic tree architecture in solving image classification problems." />
<meta property="og:description" content="In this project, we investigate biologically inspired dendritic tree architecture in solving image classification problems." />
<meta name="twitter:description" content="In this project, we investigate biologically inspired dendritic tree architecture in solving image classification problems." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Image Classification with Denritic Trees</nobr>
 <nobr class="widenobr">For CS 4440</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Can Single Neurons Solve MNIST? The
Computational Power of Biological Dendritic
Trees</h2>
 
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
<p>The paper investigates the ability of a neural network architecture inspired by the dendritic functionality of biological neurons in solving image classification
problems. Typical artificial neural network architecture involves linearly summing a set of inputs before applying a nonlinear threshhold. This is different from 
how biological neurons function. The dendrites of biological neurons introduce nonlinearities to synaptic inputs before summation at the soma. The authors of the paper 
created a neural network model that better approximates the behavior of biological neurons and trained and tested it on multiple image classification datasets. A flaw of 
authors' model is that the weights of the model can be postive or negative. In true biological neurons the weights can only be positive. In this project, we modify the 
model so that it constrains the values of the weights to only be positive. </p>

<h2>Paper Review</h2>

<p>Since the ouputs of a neuron are binary, the authors chose to train their model on a binary classification task. To do this, they went through multiple 
image classification datasets and found the least linearly seperable classes in each of them. The task of the model was then to distinguish between those two classes. 
The datasets used are MNIST (Lecun et al., 1998), Fashion-MNIST (Xiao et al., 2017), EMNIST (Cohen et al., 2017), Kuzushiji-MNIST (Clanuwat et al., 2018), 
CIFAR-10 (Krizhevsky, 2009), Street View House Numbers (SVHN) (Goodfellow et al., 2014), and USPS (Hastie et al., 2001). A fully connected neural network with two layers 
and one output node was used as a control to compare the k-tree model to. 
</p>

<p>
To implement the k-tree model, a feed-forward neural network was developed with sparse binary-tree connections, where each node connects to two inputs and produces one output. 
This model features a series of weight matrices that are initially dense tensors but are sparsified in such a way that many weights are set to zero. Specifically, for processing 
1024 pixel-size images, the model architecture consists of a 1-tree with 10 layers, where the size of the layers decreases exponentially from 1024 by 512 to 2 by 1. For larger 
3072 pixel-size images, an additional layer is included, starting from 3072 by 1024. The final layer in both configurations is structured as k by 1, reflecting the number of 
subtrees in the k-tree. The initialization of these sparsified matrices uses a Kaiming normal approach, adjusted for the density of the non-zero elements. Additionally, a "freeze mask" is 
employed to maintain the zero-value weights unchanged during training. The network utilizes leaky ReLU activation with a slope of 0.01 for intermediate nodes and a sigmoid function 
at the output to keep the final values between 0 and 1.
</p>

<p>
In the training, the authors used a Nvidia GeForce 1080 GPU with CUDA version 10.1. The model and its components were configured with a batch size of 256. An early stopping 
mechanism was implemented, where training would cease if no improvement in loss was observed over 60 epochs. Binary cross entropy loss was used to calculate loss. The Adam optimizer, 
with a learning rate of 0.001, was used for adjustments in the model's weights.

A significant step in the training process involved zeroing out gradients specified by a "freeze mask" right after the backward step (where gradients are computed) and before the weights 
are updated. This technique was used to maintain sparse connections within the model.

The experiment ran a train-test loop across 10 trials, using different subsets for training in each trial, while testing was consistently performed on the same dataset. After completing 
these trials, averages and standard deviations of the outcomes were computed, and statistical significance was assessed using Student's t-test.
</p>

<p>The model demonstrated enhanced performance in high-dimensional binary image classification tasks compared to a linear classifier and approached the performance of a fully connected 
neural network (FCNN) when the number of dendritic repetitions was increased, specifically matching FCNN in several tasks. However, the study's relevance is limited by the biologically 
implausible one-dimensional vectorized inputs used, which affected performance depending on their orderâ€”an area identified for further investigation. Moreover, the constrained dendritic 
tree model's assumptions about synaptic inputs and inter-node weights warrant reevaluation to increase biological plausibility. Future work will involve comparing the performance of this 
dendrite-inspired structure with less structured sparse ANNs and exploring the impacts of modifying input ordering and parameter constraints on model efficacy.
</p>
 
<h2>Technical Structure</h2>

<p>We replicated the authors&#39 model architecture, using a k-tree model to be a feed forward neural network with sparse binary-tree connections. We sparsified the 
 weight matrices by having each node receive 2 inputs and producing 1 output. The final layer of this tree is k by 1, k being the number of subtrees in the k-tree. 
 We used a <q>freeze mask</q> that records which weights are set to 0 in order to freeze 0-weights during training. Additionally, we used a sigmoid non-linearity at the 
 final output node which keeps output values between 0 and 1, and we used a leaky ReLU with a 0.01 slope for nodes between layers.
</p>

 <p>For baseline comparison, we also ran LDA, FCNN models, as they did in the paper. The linear discriminant
analysis (LDA) linear classifier was relatively simple, using the out of box Sklearn package. This model is relatively simple and uses dimension reduction to do linear classification. 
 This model was used to " approximate the performance of a linear point neuron model" (Jones, 2020). The fully connected neural network (FCNN) is more complex and size-comparable to parameter-complex k-tree.
  The FCNN densely connected and consists of 2-layers. With its nonlinearities, we expect
it to learn to express a greater variety of functions, therefore its performance
sets an expected upper bound.
 </p>

<p> 
 In attempts to make a more biologically-inspired neural network, we constrained the values of the weights to only be positive. We did this by applying an absolute
 value function during the standard <q>Kaiming normal</q> weight initialization. Additionally, during training, we clamp the values to be non-negative. We plan to make
 the model more biologically plausible by constraining these values to be scalar. We performed this on the 1-tree model for MNIST.
</p>

 <h2>Experimental Findings</h2>

 <img src="fig2" alt="fig2" width=600 height=300>
 <img src="fig3" alt="fig3" width=600 height=300>

 <p> 
 
</p>
 

 

 <h2>Conclusions</h2>

<h3>References</h3>

<p><a name="bottou-1990">[1]</a> <a href="https://papers.baulab.info/Bottou-1990.pdf"
  >L&eacute;on Bottou and Patrick Gallinari.
  <em>A framework for the cooperation of learning algorithms.</em></a>
  Advances in neural information processing systems 3 (1990).
</p>

<h2>Team Members</h2>

 <p>John Mathew and Ben Tunney</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
